{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain = 'TextEditing'\n",
    "domain = 'sklearn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: Reading text from test cases, case index: 1...     \n",
      "\n",
      "-- Log: query:  Linear regression\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import HISyn.front_end.front_end_function_kit as front_kit\n",
    "\n",
    "# set NL query\n",
    "# text = 'Insert colon after 1st word'\n",
    "\n",
    "# get query from test cases\n",
    "index = 1\n",
    "text = front_kit.read_text('./Documentation/' + domain + '/text_new.txt', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: Set grammar graph...     \n",
      "\n",
      "-- Log: Reading grammar file and pre-processing text...     \n",
      "\n",
      "-- Log: Grammar text preparation finished     \n",
      "\n",
      "-- Log: Start building grammar tree...     \n",
      "\n",
      "-- Log: Add API description...     \n",
      "\n",
      "-- Log: Set nodes max formal children length     \n",
      "\n",
      "-- Log: Grammar tree build finished.     \n",
      "\n",
      "-- Log: Grammar graph built     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build grammar graph\n",
    "import HISyn.domain_knowledge.DomainKnowledgeConstructor as dkc\n",
    "gg = dkc.set_grammar_graph(domain, './Documentation/' + domain + '/grammar.txt', './Documentation/' + domain + '/API_documents.txt', reload = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression ['_fit_intercept', '_normalize', '_copy_X', '_n_jobs', '_positive', '_lr_methods'] {'_linear_regression'} create a linear regression model, make a least squares model norm         \n",
      "fit_intercept_key_ ['_bool'] {'_fit_intercept'} fit the intercept if data is not centered keyword_arg         \n",
      "normalize_key_ ['_bool'] {'_normalize'} normalize, standardize, normalized data, standardized data, normalization, normalizing, standardization, standardizing keyword_arg         \n",
      "copy_X_key_ ['_bool'] {'_copy_X'} duplicate x, make a copy of x keyword_arg         \n",
      "n_jobs_key_ ['python_int'] {'_n_jobs'} number of threads, number of jobs, parallel jobs keyword_arg         \n",
      "positive_key_ ['_bool'] {'_positive'} force the coefficients to be positive keyword_arg         \n",
      "_True_ [''] {'_bool'} with, true common_knowledge         \n",
      "_False_ [''] {'_bool'} without, not, aren't, isn't common_knowledge         \n",
      "fit ['_X', '_y', '_sample_weight'] {'_lr_methods'} fit the data to a model, train the model on the data, train a linear regression norm         \n",
      "predict ['_X'] {'_lr_methods'} Predict using the linear model norm         \n",
      "get_params ['_deep'] {'_lr_methods'} Get parameters for this estimator, get parameters for the model norm         \n",
      "score ['_X', '_y', '_sample_weight'] {'_lr_methods'} Return the coefficient of determination of the prediction, score the model, evaluate the model norm         \n",
      "set_params ['_fit_intercept', '_normalize', '_copy_X', '_n_jobs', '_positive'] {'_lr_methods'} Set the parameters of the estimator, Set the parameters of the model, Set the parameters of the regression norm         \n",
      "deep_key_ ['_bool'] {'_deep'} return the parameters for this estimator and contained subobjects or submodels that are estimators,  keyword_arg         \n",
      "X_key_ ['_array_like'] {'_X'} X value of the data, Training data keyword_arg         \n",
      "y_key_ ['_array_like'] {'_y'} target values keyword_arg         \n",
      "np.array ['python_list'] {'_array_like'} array, Create an array norm         \n",
      "_sample_weight_key_ ['_array_like'] {'_sample_weight'} Individual weights for each sample common_knowledge         \n",
      "_query [['_object']] set() ['_object']          \n",
      "_object [['_linear_regression']] {'_query'} ['_linear_regression']          \n",
      "_linear_regression [['LinearRegression']] {'_object'} ['LinearRegression(_fit_intercept,_normalize,_copy_X,_n_jobs,_positive)._lr_methods']          \n",
      "_fit_intercept [['empty'], ['fit_intercept_key_']] {'LinearRegression', 'set_params'} ['empty', 'fit_intercept_key_(_bool)']          \n",
      "_normalize [['empty'], ['normalize_key_']] {'LinearRegression', 'set_params'} ['empty', 'normalize_key_(_bool)']          \n",
      "_copy_X [['empty'], ['copy_X_key_']] {'LinearRegression', 'set_params'} ['empty', 'copy_X_key_(_bool)']          \n",
      "_n_jobs [['empty'], ['n_jobs_key_']] {'LinearRegression', 'set_params'} ['empty', 'n_jobs_key_(python_int)']          \n",
      "_positive [['empty'], ['positive_key_']] {'LinearRegression', 'set_params'} ['empty', 'positive_key_(_bool)']          \n",
      "_bool [['_True_'], ['_False_']] {'deep_key_', 'normalize_key_', 'copy_X_key_', 'fit_intercept_key_', 'positive_key_'} ['_True_()', '_False_()']          \n",
      "_lr_methods [['empty'], ['fit'], ['predict'], ['get_params'], ['score'], ['set_params']] {'LinearRegression'} ['empty', 'fit(_X,_y,_sample_weight)', 'predict(_X)', 'get_params(_deep)', 'score(_X,_y,_sample_weight)', 'set_params(_fit_intercept,_normalize,_copy_X,_n_jobs,_positive)']          \n",
      "_deep [['empty'], ['deep_key_']] {'get_params'} ['empty', 'deep_key_(_bool)']          \n",
      "_X [['X_key_']] {'fit', 'predict', 'score'} ['X_key_(_array_like)']          \n",
      "_y [['y_key_']] {'fit', 'score'} ['y_key_(_array_like)']          \n",
      "_array_like [['np.array']] {'_sample_weight_key_', 'X_key_', 'y_key_'} ['np.array(python_list)']          \n",
      "_sample_weight [['None'], ['_sample_weight_key_']] {'fit', 'score'} ['None', '_sample_weight_key_(_array_like)']          \n"
     ]
    }
   ],
   "source": [
    "gg.build_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: Parsing text...     \n",
      "\n",
      "-- Log: Check NLP server status...     \n",
      "\n",
      "-- Log: NLP server already started!     \n",
      "\n",
      "test:  Linear regression\n",
      "            \n",
      "-- Log: Start parsing sentence: Linear regression     \n",
      "\n",
      "-- Log: starting JAVA Stanford CoreNLP Server...     \n",
      "\n",
      "-- Log: Parsing finished     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parsing the query and prune the unimportant edges.\n",
    "# NLP\n",
    "# get_ipython().run_line_magic('env', 'CORENLP_HOME=./third_party_pkgs/stanford-corenlp-full-2018-10-05')\n",
    "nlp = front_kit.nlp_parsing(text, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: apply domain_specific_parsing_rules...     \n",
      "\n",
      "-- Log: domain specific rules applied.     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "front_kit.domain_specfic_parsing_rules(domain, nlp, gg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: Pruning unimportant edges...     \n",
      "\n",
      "-- Log: Pruning unimportant edges...     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import HISyn.common_knowledge.NLPCommonKnowledge as nlpck\n",
    "front_kit.prune_edges(nlp, nlpck.prunable_dep_tags, nlpck.prunable_pos_tags, nlpck.common_knowledge_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Dependency graph -----------------             \n",
      "regression [NN] [O] [regression] [] --amod--> Linear [JJ] [O] [linear] [] ====>> [] []\n",
      "             \n",
      "--------------------------------------------------             \n"
     ]
    }
   ],
   "source": [
    "nlp.displayByEdge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HISyn.back_end.back_end_function_kit as back_kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: checking common knowledge     \n",
      "\n",
      "-- Log: common knowledge replace finished     \n",
      "\n",
      "-- Log: start keywords mapping     \n",
      "\n",
      "regression ['create', 'a', 'linear', 'regression', 'model', '', 'make', 'a', 'least', 'squares', 'model'] True\n",
      "regression ['fit', 'the', 'intercept', 'if', 'data', 'is', 'not', 'centered'] False\n",
      "regression ['normalize', '', 'standardize', '', 'normalized', 'data', '', 'standardized', 'data', '', 'normalization', '', 'normalizing', '', 'standardization', '', 'standardizing'] False\n",
      "regression ['duplicate', 'x', '', 'make', 'a', 'copy', 'of', 'x'] False\n",
      "regression ['number', 'of', 'threads', '', 'number', 'of', 'jobs', '', 'parallel', 'jobs'] False\n",
      "regression ['force', 'the', 'coefficients', 'to', 'be', 'positive'] False\n",
      "regression ['with', '', 'true'] False\n",
      "regression ['without', '', 'not', '', \"aren't\", '', \"isn't\"] False\n",
      "regression ['fit', 'the', 'data', 'to', 'a', 'model', '', 'train', 'the', 'model', 'on', 'the', 'data', '', 'train', 'a', 'linear', 'regression'] True\n",
      "regression ['predict', 'using', 'the', 'linear', 'model'] False\n",
      "regression ['get', 'parameters', 'for', 'this', 'estimator', '', 'get', 'parameters', 'for', 'the', 'model'] False\n",
      "regression ['return', 'the', 'coefficient', 'of', 'determination', 'of', 'the', 'prediction', '', 'score', 'the', 'model', '', 'evaluate', 'the', 'model'] False\n",
      "regression ['set', 'the', 'parameters', 'of', 'the', 'estimator', '', 'set', 'the', 'parameters', 'of', 'the', 'model', '', 'set', 'the', 'parameters', 'of', 'the', 'regression'] True\n",
      "regression ['return', 'the', 'parameters', 'for', 'this', 'estimator', 'and', 'contained', 'subobjects', 'or', 'submodels', 'that', 'are', 'estimators', '', ''] False\n",
      "regression ['x', 'value', 'of', 'the', 'data', '', 'training', 'data'] False\n",
      "regression ['target', 'values'] False\n",
      "regression ['array', '', 'create', 'an', 'array'] False\n",
      "regression ['individual', 'weights', 'for', 'each', 'sample'] False\n",
      "Linear ['create', 'a', 'linear', 'regression', 'model', '', 'make', 'a', 'least', 'squares', 'model'] True\n",
      "Linear ['fit', 'the', 'intercept', 'if', 'data', 'is', 'not', 'centered'] False\n",
      "Linear ['normalize', '', 'standardize', '', 'normalized', 'data', '', 'standardized', 'data', '', 'normalization', '', 'normalizing', '', 'standardization', '', 'standardizing'] False\n",
      "Linear ['duplicate', 'x', '', 'make', 'a', 'copy', 'of', 'x'] False\n",
      "Linear ['number', 'of', 'threads', '', 'number', 'of', 'jobs', '', 'parallel', 'jobs'] False\n",
      "Linear ['force', 'the', 'coefficients', 'to', 'be', 'positive'] False\n",
      "Linear ['with', '', 'true'] False\n",
      "Linear ['without', '', 'not', '', \"aren't\", '', \"isn't\"] False\n",
      "Linear ['fit', 'the', 'data', 'to', 'a', 'model', '', 'train', 'the', 'model', 'on', 'the', 'data', '', 'train', 'a', 'linear', 'regression'] True\n",
      "Linear ['predict', 'using', 'the', 'linear', 'model'] True\n",
      "Linear ['get', 'parameters', 'for', 'this', 'estimator', '', 'get', 'parameters', 'for', 'the', 'model'] False\n",
      "Linear ['return', 'the', 'coefficient', 'of', 'determination', 'of', 'the', 'prediction', '', 'score', 'the', 'model', '', 'evaluate', 'the', 'model'] False\n",
      "Linear ['set', 'the', 'parameters', 'of', 'the', 'estimator', '', 'set', 'the', 'parameters', 'of', 'the', 'model', '', 'set', 'the', 'parameters', 'of', 'the', 'regression'] False\n",
      "Linear ['return', 'the', 'parameters', 'for', 'this', 'estimator', 'and', 'contained', 'subobjects', 'or', 'submodels', 'that', 'are', 'estimators', '', ''] False\n",
      "Linear ['x', 'value', 'of', 'the', 'data', '', 'training', 'data'] False\n",
      "Linear ['target', 'values'] False\n",
      "Linear ['array', '', 'create', 'an', 'array'] False\n",
      "Linear ['individual', 'weights', 'for', 'each', 'sample'] False\n",
      "-- Log: keywords mapping finished     \n",
      "\n",
      "-- Log: apply domain_specific mapping rules...     \n",
      "\n",
      "-- Log: domain specific rules applied.     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "back_kit.semantic_mapping(domain, gg, nlp, nlpck.common_knowledge_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Dependency graph -----------------             \n",
      "regression [NN] [O] [regression] ['LinearRegression', 'fit', 'set_params'] --amod--> Linear [JJ] [O] [linear] ['LinearRegression', 'fit', 'predict'] ====>> [] []\n",
      "             \n",
      "--------------------------------------------------             \n"
     ]
    }
   ],
   "source": [
    "nlp.displayByEdge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: start longest matching     \n",
      "\n",
      "---------------- Dependency graph -----------------             \n",
      "regression [NN] [O] [regression] ['LinearRegression', 'fit'] --amod--> Linear [JJ] [O] [linear] [] ====>> [] []\n",
      "             \n",
      "--------------------------------------------------             \n",
      "---------------- Dependency graph -----------------             \n",
      "regression [NN] [O] [regression] ['LinearRegression', 'fit'] --amod--> Linear [JJ] [O] [linear] [] ====>> [] []\n",
      "             \n",
      "--------------------------------------------------             \n"
     ]
    }
   ],
   "source": [
    "back_kit.longest_matching(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: set preposition mappings ...     \n",
      "\n",
      "-- Log: subj reordering ...     \n",
      "\n",
      "-- Log: checking no governor dependency source     \n",
      "\n",
      "-- Log: add root token to no governor source     \n",
      "\n",
      "-- Log: remove empty edges     \n",
      "\n",
      "-- Log: empty edge removed.     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "[dependent_dict, no_gov_source_dict, empty_mapping_source_edge_dict, root_index] = back_kit.reordering(nlp, gg, nlpck.preposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: Starting all paths search ...     \n",
      "\n",
      "-- Log: singel start BFS...     \n",
      "\n",
      "-- Log: Start API:  LinearRegression    \n",
      "\n",
      "-- Log: endAPIset:  []    \n",
      "\n",
      "-- Log: path_limit:  None    \n",
      "\n",
      "-- Log: total paths:  1    \n",
      "\n",
      "-- Log: singel start BFS...     \n",
      "\n",
      "-- Log: Start API:  fit    \n",
      "\n",
      "-- Log: endAPIset:  []    \n",
      "\n",
      "-- Log: path_limit:  None    \n",
      "\n",
      "-- Log: total paths:  1    \n",
      "\n",
      "-- Log: Edges reordering...      \n",
      "\n",
      "-- Log: replace common knowledge api...     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "back_kit.reversed_all_path_searching(domain, nlp, gg, dependent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Dependency graph -----------------             \n",
      "root [] [] [] [] --root--> regression [NN] [O] [regression] ['LinearRegression', 'fit'] ====>> [['LinearRegression'], ['LinearRegression', 'fit']] []\n",
      "             \n",
      "--------------------------------------------------             \n"
     ]
    }
   ],
   "source": [
    "nlp.displayByEdge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: find siblings on dependency graph...     \n",
      "\n",
      "-- Log: attach_non_sibling_edges...     \n",
      "\n",
      "-- Log: Path selection: min path/prefix tree heuristic     \n",
      "\n",
      "-- Log: connect all cgts whose root is not root     \n",
      "\n",
      "-- Log: connect cgts to root cgts     \n",
      "\n",
      "-- Log: Set all cgt combinations ...     \n",
      "\n",
      "-- Log: Combine cgts in each group ...     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_cgt_list = back_kit.path_selection_and_combination(nlp, gg, dependent_dict, root_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: changing argument order of valid nodes     \n",
      "\n",
      "-- Log: removing trees with wrong grammar...     \n",
      "\n",
      "-- Log: adding default argument to complete the cgt...     \n",
      "\n",
      "-- Log: changing argument order of valid nodes     \n",
      "\n",
      "-- Log: converting to codes...     \n",
      "\n",
      "-- Log: converting to codes...     \n",
      "\n",
      "-- Log: promising exprs:  ['LinearRegression()']    \n",
      "\n",
      "-- Log: full candidate list:  1    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "[min_expr, expr_list] = back_kit.code_generation(gg, final_cgt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- LinearRegression()\n",
      "- LinearRegression()\n"
     ]
    }
   ],
   "source": [
    "for i in min_expr:\n",
    "    print('-', i)\n",
    "for i in expr_list:\n",
    "    print('-', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
